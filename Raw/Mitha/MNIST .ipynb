{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST DIGIT RECOGNITION USING CONVOLUTION NETWORK\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digit Recognition is an example of classification training. MNIST is a computer vision dataset which consist of 28 x 28 images of handwritten digits. \n",
    "MNIST has two main parts: \n",
    "Image of a handwritten digit, xs\n",
    "The corresponding label for the digits, ys (ranging from 0 to 9)\n",
    "\n",
    "MNIST is considered to be of low dimensional manifold as per the Manifold hypothesis which seeks a differential structure for algebraization in the space deduced from the data points. While considering the MNIST data points spread out in a 784 dimensional cube, the main objective is to find the best possible [perspective](http://colah.github.io/posts/2014-10-Visualizing-MNIST/#raw_mnist) that gives the best view of the MNIST image. [Principal Component Analysis(PCA)](http://www.cs.princeton.edu/courses/archive/spr08/cos424/scribe_notes/0424.pdf) is one technique with which this can be obtained. It deals with dimensionality reduction by reducing the number of feature points from the input data to represent it. This in turn enables faster classification, reduced requirement of the memory and an easier way to represent data. \n",
    "\n",
    "Dimensionality reduction is achieved by projecting the data point from a higher dimension to a lower dimension, in a way that error that has occured due to the reconstruction process is minimal. PCA is has its applications in industries which deals with huge amounts of data.\n",
    "Labels for MNIST dataset is denoted as one hot vector, 1 in a single dimension and 0 in other dimesnions. That is for the digit 4, the vector is [0,0,0,0,1,0,0,0,0,0]. In the mnist case, we need to classify our input points one among 10 classes ( 0 to 9). Softmax regression is one type of regression that helps in a classification which involves multiple groups. The weighted inputs of pixel intensities are calculated first, added to a bias value and are converted to probabilities, which determines the chances for the input to be in a particular class/group. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://10.241.1.85:8888/files/formulas/softmax2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model that implements softmax, a gradient descent algorithm is used to minimize the cost which is in terms of weights and bias. That is ,the gradient descent algorithm helps to find the weights and bias which gives the least cost. This is explained by considering cost as a function of 2 values w and b. The cost function used in the tutorial is cross entropy, which is obtained by getting the logarithm of the output of the softmax function. This indicates the closeness of the predicted values to the targeted output / error obtained. \n",
    "\n",
    "In back propagationalgorithm, initially during forward pass, the weighted inputs added with bias value forms the input to the hidden layer in the network, which is then modified using an activation function. Back propagation algorithm deals with error calculation which involved the target value and the output obtained. The error is used in backward pass so that the weights are updated in such a manner that the error is minimized. In short, this algorithm helps to train a neural network. The main goal of using back propagation algorithm is to help gradient descent module compute the least cost, which is a function of weight and bias.\n",
    "\n",
    "If v1 and v2 are real values variables denoting w and b, the change in cost function is : $\\Delta C = $ $\\left( \\frac{\\partial C}{\\partial v1} \\Delta v1\n",
    "      + \\frac{\\partial C}{\\partial v2} \\Delta v2\n",
    "       \\right)$\n",
    "       \n",
    "       \n",
    "Our goal is to generate $\\Delta v1$ and $\\Delta v2 $, such that it gives the least cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta C $ is the gradient vector ie; when v changes, it changes $\\Delta C$. The gradient of C is vector of partial derivatives and can also be denoted as \n",
    "$\\Delta C = \\Delta C \\Delta V $\n",
    "\n",
    "Now to make it give the least cost, $\\Delta v $ can be given as a learning parameter $\\eta$. It is used to obtain $\\Delta v = - \\eta \\Delta C$ . This while replacing the equation to get gradient vector results in a square of the cost, thus confirming the decrementing nature of cost. This describes the basis of gradient descent algorithm, which is described in detail [here](http://neuralnetworksanddeeplearning.com/chap1.html).  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation Algorithm helps to tract the training of deep models. In here, the algorithm determines how the variables are affecting the cost(suppose to be minimal). Using a learning rate of 0.01,the task of minimizing the cross entropy is undertaken by gradient descent algorithm. It is achieved by leading the bit in the direction that reduces the cost. The implementation of softmax regression, followed by its accuracy calculation as directed by [Tensor Flow tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html) documentation is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below program is as per the information provided in Tensor flow tutorial. \n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None, 784])\n",
    "#Variable: tf.Variables creates a variable. These variables are used to generate weights #and bias. Once created, they are initialised as tensors with value 0.\n",
    "#The shape [784, 10] produces a 10 dimensional vector after multiplying with 784 #dimensional vectors.\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([1,10]))\n",
    "#softmax function is applied to the result obtained after adding the bias to the product\n",
    "y = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "y_ = tf.placeholder(tf.float32,[None,10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "# The below statement initialises the weight and bias variables\n",
    "init = tf.initialize_all_variables()\n",
    "#Sessions are launched to set the model to train\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "#tf.argmax function and tf.equal function are used formodel evaluations.\n",
    "#tf.argmax function provides information about the highest entry in a tensor along axis\n",
    "#tf.equal function checks if the predicted result matches the label\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "print (y.get_shape())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an understanding, I tried adding a new layer to the above model. The objective of the below program is to take the evidence calculated to hidden layer 1 which has the sigmoid activation function. The output of which is fed to hidden layer 2 of the model with the same activation function. To get the final output it is fed to the softmax classifier and is then subjected for training using the cross entropy, gradient descent thereafter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "``` python \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None, 784])\n",
    "#Variable: tf.Variables creates a variable. These variables are used to generate weights #and bias. Once created, they are initialised as tensors with value 0.\n",
    "#The shape [784, 10] produces a 10 dimensional vector after multiplying with 784 dimensional vectors(x).\n",
    "# Hidden Layer1\n",
    "W1 = tf.Variable(tf.zeros([784,10]))\n",
    "b1 = tf.Variable(tf.zeros([1,10]))\n",
    "#softmax function is applied to the result obtained after adding the bias to the product\n",
    "y1_inter = tf.nn.sigmoid(tf.matmul(x,W1)+b1)\n",
    "# Hidden Layer2\n",
    "W2 = tf.Variable(tf.zeros([10, 10]))\n",
    "b2 = tf.Variable(tf.zeros([1,10]))\n",
    "y2_inter = tf.nn.sigmoid(tf.matmul(y1_inter,W2)+b2)\n",
    "y2 = tf.nn.softmax(y2_inter)\n",
    "# Training starts here\n",
    "y_ = tf.placeholder(tf.float32,[None,10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y2))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "# The below statement initialises the weight and bias variables\n",
    "init = tf.initialize_all_variables()\n",
    "#Sessions are launched to set the model to train\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "  correct_prediction = tf.equal(tf.argmax(y2,1), tf.argmax(y_,1))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPLYING CONVOLUTION NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor flow tutorials gives directions to do the following:\n",
    "The model uses Relu as activation fucntions forthe neurons. Rectified Linear Unit or Relu is a function that computes $f(x) = max(0,x)$ . Relu activation function accelerates convergence of stochastic gradient descent as :\n",
    "\n",
    "$ w := w - \\eta \\Delta Q_{i}(w) $\n",
    "\n",
    "where several updates are done on the training sample sets,followed by shuffling so that it converges quickly. Relu when compared with the other activation functions like tanh or sigmoid converges quickly and has much lesser complex computation involved. since relu has a negative property of haing chances of dying away irreversibly while training. For this reason, in our model the weights are inititalised with some noise to prevent zero gradients,symmetry breaking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward aritificial neural network\n",
    "Involves multiple layers of neurons that takes care of the processes done on a portion in the input image. Biologically, this region can be termed as receptive fields. Output from the receptive fields are tiled up for better original image representation. \n",
    "\n",
    "In simple terms, convolution can be considered as a window/filter/kernel that slides over each element in the input image. The window indicates a function which needs to be applied over the image. [Convolutional Neural Networks (CNN)](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/) are those with multiplelayers of convolution applied to an image. The results are fed to a non linear activation function like, Relu (in the model we are discussing in this document). Instead of a fully connected layer, CNN uses a local connection where convolution is applied to the input and it is a region from the input layer which is connected to the output layer neurons. This way, there is better performance and lesser requirement on memnory. The values of the filter that is applied to the input is learned during the training phase. \n",
    "The 2 main noteworthy aspects that is satisfed are : Invariance to transformation and compositionality. Pooling provides invariance to transformations like rotation, translation, scaling. The filter applied to input( edge detection from input pixels) composes the local region (to shape detection) to move to a higher level(like facial shape detection) that helps in classification. Pooling is applied after convolution. It gets the input subsamples. Pooling is done by applying a max operation on the convolution results. This is done so as to get a fixed size output and also to reduce dimensionality intending to keep the important features.\n",
    "\n",
    "While applying convolution to the sides of an image, narrow convolution or wide convolution can be used. Wide convolution is that where zero padding is done to the window elements that has no input pixels to act on. Narrow convolution does not use zero padding. \n",
    "\n",
    "The output obtained will take the sixe : $n_{out} = (n_{in}+2 * n_{padding}-n_{filter}+1)$ \n",
    "\n",
    "Stride size is that which denotes the number of pixels to be shifted. Larger the stride size, smaller is the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below program is done for an understanding purpose with the information given in tensor flow tutorial. There 3 hidden layers involved. Each convolution layer consists of: Convolution, Pooling and applying normalisation onto the results.  The pooling I tried here is average pooling function. \n",
    "Normalisation helps neurons from getting saturated as a result of inputs with varied scale. It gave a test accuracy of 99.09%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None, 784])\n",
    "# Initialise weight 784 x 10 matrix and bias vector with dimension 10 (0 to 9 classes) variables with zeros.\n",
    "# [784,10] is chosen because it is a 784 dimension space(28 x 28) with 10 output classes.\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# session command fills the variable nodes with zeros initially\n",
    "sess.run(tf.initialize_all_variables())\n",
    "#softmax function is applied to the result obtained after adding the bias to the product\n",
    "y = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "y_ = tf.placeholder(tf.float32,[None,10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y)))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "#init = tf.initialize_all_variables()\n",
    "for i in range(1000):\n",
    "# 50 indicates the number of iterations loaded for training purpose\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "#tf.argmax function and tf.equal function are used formodel evaluations.\n",
    "#tf.argmax function provides information about the highest entry in a tensor along axis\n",
    "#tf.equal function checks if the predicted result matches the label\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) #output is a set of boolean that are mapped to floating point numbers\n",
    "# mean is performed on that set of numbers as shown below\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "# Multilayer convolution network, requires multiple weights and bias values\n",
    "# tf.truncated_normal function generates random values from normal distributions; within the specified std deviation\n",
    "# Shape argument is 1-D array; Output values will also be in 1-D array\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "# Bias value is initialised with positive values to eradicate chances of getting dead neurons from Relu activation function\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "# Does a 2-D convolution using x and W which is 4-D tensors and returns a tensor of similar type as input\n",
    "# Strides are a list of 1-D integers of length 4; padding indicates the padding algorithm to be used-SAME,VALID\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# Pooling Operation works on a rectangular window- does reduction operation like average/max etc\n",
    "# window size depends on ksize argument and strides offset arugment\n",
    "def avg_pool_2x2(x):\n",
    "  return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def local_response_normalization(val):\n",
    "        return tf.nn.local_response_normalization(val)\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "# the below command does the max pool operation to the input tensor\n",
    "h_pool1 = avg_pool_2x2(h_conv1)\n",
    "norm1 = local_response_normalization(h_pool1)\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = avg_pool_2x2(h_conv2)\n",
    "W_conv3 = weight_variable([5,5,64,96])\n",
    "b_conv3 = bias_variable([96])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2,W_conv3)+b_conv3)\n",
    "h_pool3 = avg_pool_2x2(h_conv3)\n",
    "#h_poolnew = tf.reshape(h_pool3,[7,7])\n",
    "norm1 = local_response_normalization(h_pool3)\n",
    "W_fc1 = weight_variable([4 * 4 * 96, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool3_flat = tf.reshape(norm1, [-1, 4*4*96])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "#print(\"pool3\",h_pool3.get_shape())\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
