{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe Classification Example\n",
    "### Extracted and enhanced from [Caffe's Documentation](https://github.com/BVLC/caffe/blob/master/examples/00-classification.ipynb)\n",
    "\n",
    "\n",
    "Caffe's [Github repo provides examples](https://github.com/BVLC/caffe/tree/master/examples) on Classification using Caffe.\n",
    "This tutorial covers Caffe's native tutorial. Further, my own code and enhancements are included to better understand the working of Caffe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us import some libraries that are required to visualize the trained Neural net. These include the Numpy library for saving the trained images as arrays and Matplotlib for plotting various figures and graphs out of it.\n",
    "We also tune the plot parameters as mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up Python environment: numpy for numerical routines, and matplotlib for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# set display defaults\n",
    "# these are for the matplotlib figure's.\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caffe's examples are hosted in an example directory inside its root directory. We should run the following python code from the example directory to make sure that the scripts work. We include the *sys* and *os* modules to work with the file paths and the working directory. The caffe's python folder must be fixed as the python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The caffe module needs to be on the Python path;\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# set the working directory to caffe root directory.\n",
    "os.chdir('/root/caffe/examples/')\n",
    "\n",
    "caffe_root = '../'\n",
    "#caffe_root = '/root/caffe/'  # The caffe_root is changed to reflect the actual folder in the server.\n",
    "sys.path.insert(0, caffe_root + 'python') # Correct the python path\n",
    "\n",
    "import caffe\n",
    "# Successfully imported Caffe !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model is computationally expensive and time consuming. For this example, let us stick on to a pre-trained network bundled with Caffe. We will search for the caffemodel and start from there. Caffemodel is the trained model. During the training phase, on the set time intervals or iterations, Caffe saves caffemodel file which saves the state of the net at that particular time. For example, if we have a total of 10000 iterations to perform and we explicitly mention that we need to save the state of the net at intervals of 2000 iterations, Caffe will generate 5 caffemodel files and 5 solver files, which saves the respective state of the net at iterations 2000,4000,6000,8000 and 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out the current working directory.\n",
    "print os.getcwd()\n",
    "\n",
    "# If the reference model is not found, download it from the Caffe repo's.\n",
    "if os.path.isfile(caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'):\n",
    "    print 'CaffeNet found.'\n",
    "else:\n",
    "    print 'Downloading pre-trained CaffeNet model...'\n",
    "    !../scripts/download_model_binary.py ../models/bvlc_reference_caffenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we explicitly fixed the working directory, it is not required to have the Notebook in the same directory as that of the example. We define the Model definitions and weights of the pre-trained network by including the correct path. The neural net is defined by using the defenitions and weigths saved earlier. Since the network was already trained for a huge dataset, we can choose the Test mode in caffe and not perform dropout's while defining the net. More info on dropout [here](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set Caffe mode as CPU only. This is to be done because OCI servers are not equipped with GPU's yet.\n",
    "caffe.set_mode_cpu()\n",
    "\n",
    "# set the model definitions since we are using a pretrained network here.\n",
    "# this protoype definitions can be changed to make significant changes in the learning method.\n",
    "model_def = caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt'\n",
    "model_weights = caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'\n",
    "\n",
    "net = caffe.Net(model_def,      # defines the structure of the model\n",
    "                model_weights,  # contains the trained weights\n",
    "                caffe.TEST)     # use test mode (e.g., don't perform dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a transformer to input data into our net. Input data here are images. The subtracted-mean of the images in the dataset considered are to be set in the transformer. Mean subtraction is a way of preprocessing the image. The mean is subtracted across every individual feature in the dataset. This can be interpreted as the centering of a cloud of data around the origin along every dimension. With our input data fixed as images, this relates to subtracting the mean from each of the pixels, seperately across the three channels. More on it [here](http://cs231n.github.io/neural-networks-2/).\n",
    "\n",
    "These are the steps usually carried out in each transformers:\n",
    "1. Transpose the data from (height, width, channels) to (channels, width, height)\n",
    "2. Swap the color channels from RGB to BGR\n",
    "3. Subtract the mean pixel value of the training dataset (unless you disable that feature).\n",
    "\n",
    "More information on these [here](https://groups.google.com/forum/#!topic/digits-users/FIh6VyU1XqQ), [here](https://github.com/NVIDIA/DIGITS/issues/59) and [here](https://github.com/NVIDIA/DIGITS/blob/v1.1.0/digits/model/tasks/caffe_train.py#L938-L961)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the mean ImageNet image (as distributed with Caffe) for subtraction\n",
    "mu = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "mu = mu.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "print 'mean-subtracted values:', zip('BGR', mu)\n",
    "\n",
    "# create transformer for the input called 'data'\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, we can reshape the data to meet our specifications. In the particular example the batch size, number of channels and image size is explicitly specified as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the size of the input (we can skip this if we're happy\n",
    "#  with the default; we can also change it later, e.g., for different batch sizes)\n",
    "net.blobs['data'].reshape(50,        # batch size\n",
    "                          3,         # 3-channel (BGR) images\n",
    "                          227, 227)  # image size is 227x227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any image can be now loaded into caffe. For simplicity let us now stick with Caffe's example image ($CAFFE_ROOT/examples/images/cat.jpg). The image is then transformed as mentioned above using the transformer that we defined. Finally, the image is plotted using matplotlib.pyplot imported as plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = caffe.io.load_image(caffe_root + 'examples/images/cat.jpg')\n",
    "transformed_image = transformer.preprocess('data', image)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! Now we have our net ready so is the image that we need to classify. Remember that data in Caffe is interpreted using blobs. Quoting from [Caffe's Documentation](http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html) : *As data and derivatives flow through the network in the forward and backward passes Caffe stores, communicates, and manipulates the information as blobs: the blob is the standard array and unified memory interface for the framework.* \n",
    "\n",
    "For caffe to get information from the image, it needs to be copied to the memory allocated by Caffe. \n",
    "\n",
    "Once the image is loaded into memory, we can perform classification with it. To start classification, we call ***net.forward()*** and redirect its output to a variavle named output (name can be anything obviously). The probability of the output is saved in a vector format. Since we gave a batch size of 50, there will be 50 input images at once. The probability of our image will be saved in the **[0]**th location. The output probability can be extracted out by properly calling it. Finally the predicted class of the image can be extracted by using argmax which returns the indices of the maximum values along an axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy the image data into the memory allocated for the net\n",
    "net.blobs['data'].data[...] = transformed_image\n",
    "\n",
    "### perform classification\n",
    "output = net.forward()\n",
    "\n",
    "output_prob = output['prob'][0]  # the output probability vector for the first image in the batch\n",
    "\n",
    "print 'predicted class is:', output_prob.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, for the cute cat, the predicted class is 281. Make sure that you are getting the same (just in case).\n",
    "\n",
    "For our eyes the image is a cute cat, agreed. To see what our net thinks it is, let us fetch the label of the predicted/classified image. Load the labels file from the dataset and output the specific label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load ImageNet labels\n",
    "\n",
    "labels_file = caffe_root + 'data/ilsvrc12/synset_words.txt'\n",
    "\n",
    "if not os.path.exists(labels_file):\n",
    "    !/root/caffe/data/ilsvrc12/get_ilsvrc_aux.sh\n",
    "    \n",
    "labels = np.loadtxt(labels_file, str, delimiter='\\t')\n",
    "\n",
    "print 'output label:', labels[output_prob.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the prediction ? Fair ? Let us see a quantitative result. We will output the top five predictions from the output layer (softmax layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort top five predictions from softmax output\n",
    "top_inds = output_prob.argsort()[::-1][:5]  # reverse sort and take five largest items\n",
    "\n",
    "print 'probabilities and labels:'\n",
    "zip(output_prob[top_inds], labels[top_inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the time required to train the network for the particular input, let us use timeit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the time required to train the network\n",
    "%timeit net.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**blob.data.shape** can be used to find the shape of the different layers in your net. Loop across it to get shape of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each layer, show the output shape\n",
    "for layer_name, blob in net.blobs.iteritems():\n",
    "    print layer_name + '\\t' + str(blob.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer_name, param in net.params.iteritems():\n",
    "    print layer_name + '\\t' + str(param[0].data.shape), str(param[1].data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers can be viewed in a diagramatic view by exporting it to an image file. This can be done by a script available in /python folder in caffe's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! $CAFFE_ROOT/python/draw_net.py $CAFFE_ROOT/models/bvlc_reference_caffenet/deploy.prototxt caffenet.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vis_square(data):\n",
    "    \"\"\"Take an array of shape (n, height, width) or (n, height, width, 3)\n",
    "       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n",
    "    \n",
    "    # normalize data for display\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    \n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = (((0, n ** 2 - data.shape[0]),\n",
    "               (0, 1), (0, 1))                 # add some space between filters\n",
    "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=1)  # pad with ones (white)\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "    plt.imshow(data); plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter out the weights and biases of each layer to visualize the changes happening in each layer. This is a powerful way of analyzing the net as it gives intuition into what is happening inside it. The image drawn above (layer by layer representation) together with the visualization of things happening in each layer will help us understand the net in more depth. Here we use the **conv1** layer for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the parameters are a list of [weights, biases]\n",
    "filters = net.params['conv1'][0].data\n",
    "filters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you noticed, the shape of the filter is different from the function vis_square. So we need to transpose the vector accordingly before passing it into the function to visualize the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vis_square(filters.transpose(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the data as such, we can use the net.blobs instead of net.params. This will give us a visual clue on what the data may look like and what is happening at the same time. We are doing it for the **conv1** layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = net.blobs['conv1'].data[0, :36]\n",
    "vis_square(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for the **pool5** layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = net.blobs['pool5'].data[0]\n",
    "vis_square(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot graphs using the various data saved in the layers. The fully connected layer fc6 will result in the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = net.blobs['fc6'].data[0]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(feat.flat)\n",
    "plt.subplot(2, 1, 2)\n",
    "_ = plt.hist(feat.flat[feat.flat > 0], bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of predicting the correct label for the particular image we classified can be plotted as well. X-axis is the Feature's label number and Y-Axis is the probability of correct classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = net.blobs['prob'].data[0]\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(feat.flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us download an image of our own and try to classify it. Here, a http link of the image is used to download the image. The image is then loaded into Caffe. The image is then preprocessed using the transformer we defined earlier.\n",
    "\n",
    "Once we are done with the preprocessing, we have a formated image in the memory that is ready to be classified. Perform the classification by running **net.forward()**. The output probability can be found just like earlier.The top 5 probabilities are dound out, the image displayed and the 5 probabilities are printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download an image\n",
    "# for example:\n",
    "# my_image_url = \"https://upload.wikimedia.org/wikipedia/commons/b/be/Orang_Utan%2C_Semenggok_Forest_Reserve%2C_Sarawak%2C_Borneo%2C_Malaysia.JPG\"\n",
    "#my_image_url = \"https://www.petfinder.com/wp-content/uploads/2012/11/140272627-grooming-needs-senior-cat-632x475.jpg\"  # paste your URL here\n",
    "#my_image_url =\"http://kids.nationalgeographic.com/content/dam/kids/photos/animals/Mammals/H-P/lion-male-roar.jpg\"\n",
    "\n",
    "my_image_url =\"http://www.depositagift.com/img/bank_assets/Band-Aid.jpg\"\n",
    "\n",
    "!wget -O image.jpg $my_image_url\n",
    "\n",
    "# transform it and copy it into the net\n",
    "image = caffe.io.load_image('image.jpg')\n",
    "net.blobs['data'].data[...] = transformer.preprocess('data', image)\n",
    "\n",
    "# perform classification\n",
    "net.forward()\n",
    "\n",
    "# obtain the output probabilities\n",
    "output_prob = net.blobs['prob'].data[0]\n",
    "\n",
    "# sort top five predictions from softmax output\n",
    "top_inds = output_prob.argsort()[::-1][:5]\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "print 'probabilities and labels:'\n",
    "zip(output_prob[top_inds], labels[top_inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever wonder what are the labels in the dataset, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip(labels[:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
